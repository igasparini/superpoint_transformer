# @package _global_

# to execute this experiment run:
# python train.py experiment=semantic/sensaturban

defaults:
  - override /datamodule: semantic/sensaturban.yaml
  - override /model: semantic/spt-2.yaml
  - override /trainer: gpu.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

dataloader:
    sample_graph_k: 1
    sample_graph_r: 5
    batch_size: 1
    sample_segment_ratio: 0.2
    max_num_nodes: 12500
    max_num_edges: 250000
    sample_point_min: 16
    sample_point_max: 64

callbacks:
  gradient_accumulator:
    scheduling:
      0:
        8  # accumulate gradient every 2 batches, to make up for reduced batch size

trainer:
  max_epochs: 100  # ADAPT THIS -- from s3dis: to keep same nb of steps: 8x more tiles, 2-step gradient accumulation -> epochs/4

# trainer:
#   max_epochs: 400

# Mini dataset to debug
#mini: True

model:
  optimizer:
    lr: 0.01
    weight_decay: 1e-4

logger:
  wandb:
    project: "spt_sensaturban"
    name: "SPT-64"